In this chapter, we analize the behaviors of the current TaskAffinity
implementation considering different architectures and benchmarks.  This
analysis is two fold: on one side it allows to understand why the current
task-affinity implementation is not effective on some architectures, e.g. the
Intel Xeon, on the other which are the points of task-affinity logic that can be
optimized in order to improve an application \emph{throughput and
predictability}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Scheduler architecture on 2.6.34}

Now I will briefly introduce which are the part of scheduling procedure 
interested by task-affinity logic and which are the most important changes 
carried out from 2.6.31 kernel version to 2.6.34.

%-----------------------------------------------------------------------------
\subsection{Task wake up management}

The scheduling procedure for a task starts when it wakes up. A task can wake up
for different reasons, i.e. a semaphore becomes unlocked or a new task creation.  In all those cases different
kernel functions are called, but at the end they call the
\texttt{try\_to\_wake\_up} function:

\lstset{basicstyle=\footnotesize, language=c, captionpos=b, frame=single, label=lis:APIttwu}
\lstinputlisting{API_ttwu.c}

where $p$ is the to be waken task.
This function follows these steps:

\begin{enumerate}
\item Disables kernel preemption, locks the runqueue where $p$ was last executed and check 
if $p$ is not already waken and it is not already on a runqueue. In the first case the 
function releases lock and exit, in the second case the function check if a
\textit{push} is necessary, further details about these if statements will be
describe thereafter. If two checks fail, $p$'s state is changed in TASK\_WAKING, the
lock on runqueue is released and \texttt{select\_task\_rq}, a wrapper for a 
class-specific \texttt{select\_task\_rq\_rt}, is called. The function 
\texttt{task\_waking} at line 2420 is class-specific and regards only Fair tasks.

\begin{figure}[h]
  \lstset{basicstyle=\footnotesize, language=c, captionpos=b, frame=single,label=lis:steps}
  \lstinputlisting{ttwu_steps.c}
  \label{code:ttwu}
  \caption{A portion of the \texttt{try\_to\_wake\_up} method}
\end{figure}

\item \texttt{select\_task\_rq\_rt} choose on which cpu $p$ will be executed. It
calls \texttt{find\_lowest\_rq} that returns the best cpu where to put $p$. Criteria
used to choose the best cpu for $p$ will be described soon. When \texttt{select\_task\_rq\_rt} 
returns, check for cpuaffinity \footnote{On Linux it is possible decide on which
cpu a task can be executed. The set of cpus that can execute a task is called 
cpuaffinity of that task. Each task owns a mask called \textit{cpus\_allowed} 
that include all cpus where it can be executed, that is its cpuaffinity} and 
if selected cpu is online, in that case returns, otherwise calls 
\texttt{select\_fallback\_rq} that returns an any online cpu that "respects" 
$p$'s cpuaffinity.

\begin{figure}[h]
  \lstset{basicstyle=\footnotesize, language=c, captionpos=b, frame=single,label=lis:steps}
  \lstinputlisting{select_task.c}
  \label{code:ttwu}
  \caption{A portion of the \texttt{???} method}
\end{figure}

\item acquires the lock on selected runqueue, updates some $p$'s statistics, enqueues 
$p$ on selected runqueue and call \texttt{check\_preempt\_rq} 

\begin{figure}[h]
  \lstset{basicstyle=\footnotesize, language=c, captionpos=b, frame=single,label=lis:steps}
  \lstinputlisting{ttwu_check.c}
  \label{code:ttwu}
  \caption{A portion of the \texttt{???} method}
\end{figure}

\item checks if $p$ has priority greater than priority of the task currently
executed on selected runqueue, in that case it calls \texttt{need\_resched}
function in order to perform the context-switch on selected runqueue at the
end of \texttt{try\_to\_wake\_up}.

\begin{figure}[h]
  \lstset{basicstyle=\footnotesize, language=c, captionpos=b, frame=single,label=lis:steps}
  \lstinputlisting{check_prio.c}
  \label{code:ttwu}
  \caption{A portion of the \texttt{???} method}
\end{figure}

\item update $p$'s state in TASK\_RUNNING and call class-specific function 
\texttt{task\_woken} to check if $p$ must be pushed from the selected runqueue.
\texttt{task\_woken} has effects only for Real-time tasks.

\begin{figure}[h]
  \lstset{basicstyle=\footnotesize, language=c, captionpos=b, frame=single,label=lis:steps}
  \lstinputlisting{final_ttwu.c}
  \label{code:ttwu}
  \caption{A portion of the \texttt{???} method}
\end{figure}

\end{enumerate}

The most important differences from version 2.6.31 related to Real-time tasks regard principally \texttt{try\_to\_wake\_up}.

It is possible to have multiple istances of \texttt{try\_to\_wake\_up} for the same task executed simultaneously. In the 2.6.31 kernel version, this problem
is resolved by holding runqueue lock. In the 2.6.34 kernel version, to deal with this issue a new task's state named TASK\_WAKING was introduced. 

\begin{figure}[h]
  \lstset{basicstyle=\footnotesize, language=c, captionpos=b, frame=single,label=lis:steps}
  \lstinputlisting{state_list.c}
  \label{code:ttwu}
  \caption{A portion of the \texttt{???} method}
\end{figure}

TASK\_WAKING is used to indicate someone is already waking the task, in this way other instances of \texttt{try\_to\_wake\_up} fails when executes the if 
statement at line TODO ref of \texttt{try\_to\_wake\_up}, because input parameter $state$ of \texttt{try\_to\_wake\_up} is in the most cases equal to 
TASK\_ALL and then, according to fig TODO stati, \texttt{TASK\_WAKING \& TASK\_ALL} return 0 and \texttt{try\_to\_wake\_up} exits. With this solution 
it is possible reduce time in which lock on runqueue is held. 


%-----------------------------------------------------------------------------
\subsection{Migration policy}

Another important part of scheduling procedure is the migration policy. Migration of Real-time tasks is made in two way: 

\begin{description}
\item[Push tasks:] The push operation is implemented by \texttt{push\_rt\_task()}. The function receives in input a runqueue and looks at the 
highest-priority non-running runnable real-time task on the input runqueue and considers all the runqueues to find a cpu where it can run. It searches for 
a runqueue that is of lower priority, that is, one where the currently running task can be preempted by the task that is being pushed. 

The research and the choice of the best cpu for the task to push is executed by \texttt{find\_lowest\_rq} the same function used in 
\texttt{select\_task\_rq\_rt}. This function builds a mask of cpus that have the lowest-priority runqueues and returns the cpu on which the task to push is 
last executed, as it is likely to be cache-hot in that location. If this is not possible, the \texttt{sched\_domain} map is considered to find a CPU that 
is logically closest to last cpu that has executed the task to push. If this too fails, a cpu is selected at random from the mask.

The push operation is performed until a real-time task fails to be migrated or there are no more tasks to be pushed. Because the algorithm always selects 
the highest non-running task for pushing, the assumption is that, if it cannot migrate it, then most likely the lower real-time tasks cannot be migrated 
either and the search is aborted. No lock is taken when scanning for the lowest-priority runqueue. When the target runqueue is found, only the lock of that 
runqueue is taken, after which a check is made to verify whether it is still a candidate to which to push the task (as the target runqueue might have been 
modified by a parallel scheduling operation on another CPU). If not, the search is repeated for a maximum of three tries, after which it is aborted. 

In order to decide which tasks must be pushed, a linked list named \textit{pushable\_list} is added to each runqueue. \texttt{push\_rt\_task()} selects tasks
to push from this list. A task is inserted in this list when it is enqueued on a runqueue as show in the snippet below.

\begin{figure}[h]
  \lstset{basicstyle=\footnotesize, language=c, captionpos=b, frame=single, label=lis:steps}
  \lstinputlisting{enqueue_push.c}
  \label{code:ttwu}
  \caption{A portion of the \texttt{???} method}
\end{figure}

The current task of any runqueue can't never be in a pushable list, in fact, during a context switch the next task to be executed is removed from the 
runqueue's pushable list.

\item[pull task:] The pull operation is implemented by \texttt{pull\_rt\_task()}. The algorithm looks at all the overloaded runqueues in the system 
and checks whether they have a Real-time task that can run on the target runqueue (that is, checks if the target cpu "respects" the cpuaffinity of the 
task to pull) and if that Real-time task is of a priority higher than the task the target runqueue is about to schedule. If so, the task is queued on 
the target runqueue. This search aborts only after scanning all the overloaded runqueues in the system. 

\end{description}

In the 2.6.34 kernel version, the migration logic and all data structures involved are not changed from 2.6.31 version.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Test computers and benchmarks}

In this section I will briefly described machines and benchmark used to test task-affinity.

\begin{figure}[htbp]
 \centering%
 \subfigure[Machine A \label{fig:i7}]%
  {\includegraphics[width=6cm,height=8cm, keepaspectratio]{images/Xeon.eps}} \qquad\qquad
 \subfigure[Machine B \label{fig:Xeon}]%
  {\includegraphics[width=6cm,height=8cm, keepaspectratio]{images/i7.eps}}
 \caption{\figurecaption{miss rate Xeon}}
\end{figure}

\begin{description}
\item[Machine A] The first machine is an Intel Xeon E5440 running at 2.83GHz. There is not any cache shared among all cores, but instead 
there are big L2-caches, of 6MB each, shared between sets of 2 cores cache hierarchy is shown in Fig TODO

\item[Machine B] The second machine is an Intel Core i7 870 processor. It runs at 2.93 GHz and has the cache configuration as illustrated in Fig. TODO
It has one L3 of 8MB, which is shared by all cores. The L2-caches are private to each processor. 

\item[Machine C] TODO

\end{description}

Benchmark used is the same used in \cite{lcs}, in Fig.\ref{fig:bench} is
represented its structure.\begin{figure}[h]

  \label{code:ttwu}
  \caption{A portion of the \texttt{???} method}
\end{figure}


\begin{figure}[htbp]
\centering
\includegraphics[width=\widefigure]{images/bench.eps}
\caption{\figurecaption{Structure of benchmark used: task are green coloured, buffer are blue coloured}}
\label{fig:bench}
\end{figure}

Execution of benchmark is divided in three steps:

\begin{enumerate}
\item Waves write their buffer
\item Mixers read data from two buffers filled by waves, they mix read data and write them on their buffer, for example mixer0 read data from buffer filled 
by wave0 and from buffer filled by wave1, mix data and then write its buffer.
\item Last mixer, reads data from buffers written by mixer0 and mixer1, mix data and write its buffer.
\end{enumerate}

When mixer2 has finished to write data on its buffer, we say that a \textit{sample} was produced. The execution time to produce a sample depends on buffer 
dimension, because each task has to fill its buffer. Note that waves fill their buffer with integer of 2 byte, therefore if buffer is of 4KB they will
write 2048 integers in their buffer. Buffer dimension is always a power of 2.
The metric used to evaluate benchmark performance is the same used in \cite{lcs}: $average + 2*standard deviation$ (A2S), where \textit{average} is the 
average of execution time to produce a sample and \textit{standard deviation} is the standard deviation of execution time to produce a sample. With this 
metric is possible measure throughput and predictability of the application.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analysis of Taskaffinity behaviour}

This section analyzes which are the improvements and worsening that current version of task-affinity involves on different test computers at the buffer 
dimension change.

In the following experiments only the Intel Xeon and Intel i7 architectures are
considered, because their cache architectures are similar in structure.
These architectures differ in inter-chip communication, the former uses Quick-path Interconnect (QPI) the latter use Hyper-transport TODO freq bus. 
Furthermore, Intel i7 use an inclusive LLC with MESIF protocol, while AMD use an exclusive LLC with MOESI protocol. To observe how these factors impact 
on task-affinity a complex analysis is required and it is not the goal of this thesis. Cache architectures of Intel Xeon and Intel i7 are greatly 
different, furthermore, also in this case, technologies used for inter-chip communication are different, therefore these processor are suitable to show 
impact of cache architecture on task-affinity.

%-----------------------------------------------------------------------------
\subsection{Application's performance}

The length of buffer determine how long is the work executed by producers and consumers. It was showed in \cite{lcs} that if buffer used is too short and 
consequently tasks have very few work to do in user-space side, the parallelism provided by the SMP system is not well profited. For this reason, in 
\cite{lcs} a buffer of 4KB was used, in this way, parallelism was well profited. In the following graphics, we compare vanilla and current version 
of task-affinity in terms of predictability and throughput on different architectures and using different buffer dimensions.

\begin{figure}[htbp]
\centering
\includegraphics[width=\widefigure]{images/time/time_avg_var_Xeon.eps}
\caption{\figurecaption{Average and Variance of execution time of a sample on Xeon}}
\label{fig:avg_var_xeon}
\end{figure}

\newpage

\begin{figure}[htbp]
\centering
\includegraphics[width=\widefigure]{images/time/time_avg_var_i7.eps}
\caption{\figurecaption{Average and Variance of execution time of a sample on i7}}
\label{fig:avg_var_i7}
\end{figure}

\begin{table}[htbp]
\begin{center}
\begin{tabular}{l|c|c|c}
	\hline
	& Xeon (Speedup) & i7 (Speedup) \\ \hline
	$4KB$  & -1.89\% & 2.33\% \\ \hline
	$8KB$  & -0.82\% & 6.42\% \\ \hline
	$16KB$ & -0.31\% & 8.42\% \\ \hline
	$32KB$ & -3.96\% & 6.53\% \\ \hline
	$64KB$ & -3.25\% & -5.14\% \\ \hline
\end{tabular}
\caption{Speedup obtained with task-affinity on different architectures with different buffer}
\label{tab:speedup_xeon_i7}
\end{center}
\end{table}

Speedup on Table \ref{tab:speedup_xeon_i7} is calculated in this way:

\begin{equation}
        \frac{A2S_{task\_affinity} - A2S_{vanilla}}{A2S_{vanilla}}
\label{eq:miss_rate}
\end{equation}

As shown in graphics and as summarized in Table \ref{tab:speedup_xeon_i7}, current version of task-affinity is not effective on Intel Xeon.
In term of throughput task-affinity is not better than vanilla, because the average of execution time of a sample is about the same in both kernels and this
is true for both the architectures. In term of predictability, on Intel i7, task-affinity is better than vanilla, especially with buffer greater than 32KB.
Task-affinity should improve predictability of application, why does this fact doesn't happen, on Xeon?

%-----------------------------------------------------------------------------
\subsection{Migration issues and cache misses}

Cache misses are an important factor that influence predictability of an application. The following graphics visualize cache misses obtained on Intel 
Xeon and on Intel i7.

\begin{figure}[htbp]
 \centering
  \includegraphics[width=\widefigure]{images/cache_miss/l1_load_store_Xeon.eps}
  \label{fig:l1_load_store_Xeon}
 \caption{\figurecaption{percentage of read and write misses on Xeon}}
\end{figure}

\begin{figure}[htbp]
 \centering
  \includegraphics[width=\widefigure]{images/cache_miss/l1_load_store_i7.eps}
  \label{fig:l1_load_store_Xeon}
 \caption{\figurecaption{percentage of read and write misses on i7}}
\end{figure}

As we can see, task-affinity doesn't reduce miss rate on Xeon. An important factor that influence cache miss is migration of tasks. Unfortunately, current 
version of task-affinity is not very effective in term of reduction of task's migrations. As already described in \cite{lcs}, during the benchmark's 
execution, \textit{mixer0} or \textit{mixer1} bounce from two different cpus. This problem is not related to the architecture or buffer dimension, it is 
an issue related to the logic of task-affinity. To understand the reason of this problem see Fig. \ref{fig:migr_pat}. 

\begin{figure}[htbp]
\centering
\includegraphics[width=\widefigure]{images/migr_i7.eps}
\caption{\figurecaption{Step A: mixer0 chooses cpu4. Step B: mixer2 chooses cpu4. Step C: mixer0 chooses cpu2}}
\label{fig:migr_pat}
\end{figure}

\newpage
Consider step A. According to task-affinty logic, \textit{mixer0} has to choose a cpu that have executed \textit{wave0} or \textit{wave1}, that is
CPU2 or CPU4, \textit{mixer2} has to choose a cpu that have executed \textit{mixer0} or \textit{mixer1}, that is CPU3 or CPU1. \textit{mixer0} chooses the 
least loaded runqeue, therefore it chooses CPU4. At Step B \textit{mixer2} can choose between CPU4 or CPU1, they have the same number of Real-time task, 
therefore \textit{mixer2} choose the first cpu in its list, that is CPU4. At step C, \textit{mixer0} has to choose CPU2 or CPU4 again, it can't choose CPU4,
as in step A, because it is still occupied by \textit{mixer2}, so \textit{mixer0} has to be executed on CPU2. This is the problem. Since \textit{mixer0} or 
\textit{mixer1} have to always choose a cpu occupied by \textit{mixer2}, one of them will have to migrate.

Task's migration can degrade performance because a migrated task could warm up a new cache and it could create new cache interference in a new location 
already occupied by other tasks. In order to measure how much this migration pattern increase miss rate of the application, this experiment is performed: 
two run of benchmark are performed, in the first run all tasks are pinned on a specific cpu and they can't mirgate, in the second run only waves are pinned
and mixers can migrate, Tab.\ref{tab:assignment} summarize cpu assignment. With this experiment it is possible to observe how task migration influences 
miss rate of the application.
\newpage

\begin{table}[htbp]
\centering%
\subfigure[All task pinned]{%
 \begin{tabular}{c|c}
	\hline
	Task & CPU \\ \hline
	Wave0 & 1  \\ \hline
	Wave1 & 2 \\ \hline
	Wave2 & 3 \\ \hline
	Wave3 & 4 \\ \hline
	Mixer0 & 1 \\ \hline
	Mixer1 & 3 \\ \hline
	Mixer2 & 3 \\ \hline
 \end{tabular} 
 \label{tab:all_pinned}
}\hspace{4em}
\subfigure[Waves pinned]{%
 \begin{tabular}{c|c}
	\hline
	Task & CPU \\ \hline
	Wave0 & 1  \\ \hline
	Wave1 & 2 \\ \hline
	Wave2 & 3 \\ \hline
	Wave3 & 4 \\ \hline
 \end{tabular} 
 \label{tab:cpuaff_waves}
}
\label{tab:assignment}
\caption{cpu assignment performed}
\end{table}

In the following graphics are showed results of the experiment. Miss rate on LLC is analyzed only for buffer dimension greater or equal to 32KB, because 
with buffer dimension less than 32KB, read and write accesses to LLC are very different between cases with all task pinned and case with only waves pinned 
and, for this reason, miss rates are not longer comparable.

\begin{figure}[htbp]
\centering
\includegraphics[width=\widefigure]{images/cpuaff/cpuaff_l1_load_store_Xeon.eps}
\caption{\figurecaption{percentage of L1 read and write misses on Xeon}}
\label{fig:cpuaff_l1_load_store_xeon}
\end{figure}
\newpage
\begin{figure}[htbp]
\centering
\includegraphics[width=\widefigure]{images/cpuaff/cpuaff_acc_l2_load_store_Xeon.eps}
\caption{\figurecaption{number of LLC read and write accesses on Xeon}}
\label{fig:cpuaff_acc_l2_load_store_xeon}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\widefigure]{images/cpuaff/cpuaff_l2_load_store_Xeon.eps}
\caption{\figurecaption{number of LLC read and write accesses on Xeon}}
\label{fig:cpuaff_l2_load_store_xeon}
\end{figure}

On Intel Xeon, migration pattern increase in particular L1 write miss. The increment of L1 write miss, cause an increment of write 
access to LLC. Regarding LLC miss rate, we can see that migration pattern increase both LLC read misses and write misses. 
\newpage

\begin{figure}[htbp]
\centering
\includegraphics[width=\widefigure]{images/cpuaff/cpuaff_l1_load_store_i7.eps}
\caption{\figurecaption{percentage of L1 read miss on i7}}
\label{fig:cpuaff_l1_load_xeon}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\widefigure]{images/cpuaff/cpuaff_acc_l3_load_store_i7.eps}
\caption{\figurecaption{number of LLC read and write accesses on Xeon}}
\label{fig:cpuaff_acc_l2_load_store_xeon}
\end{figure}

\begin{figure}[]
\centering
\includegraphics[width=\widefigure]{images/cpuaff/cpuaff_l3_load_store_i7.eps}
\caption{\figurecaption{percentage of LLC read miss on i7}}
\label{fig:cpuaff_l1_load_xeon}
\end{figure}
\newpage

On Intel i7, migration pattern increases in particular L1 read miss rate, and consequently the read accesses to LLC. Regarding LLC 
miss rate, we can see that read and write misses rate are increased.

We can infer from these graphics, that migration pattern have a great influence on read/write miss rate on LLC in both architectures considered. On Intel 
Xeon, as might be expected, L1 write misses are greatly increased, because if a mixer bounce from one cpu to another at each sample, it will hardly 
find its private buffer in a L1 cache, therefore it will have to warm up cache at each migration creating possible inter-thread interference.

On Intel i7, instead, write misses are not increased a lot. It is clear, that this fact is due to CPU architecture. To get a sense on how much cache miss 
influence predictability of the application, see Fig. \ref{fig:time_cpf_var_Xeon_i7}. In the graphics two run of benchmark are compared: in the first run 
all tasks are pinned, while in the second run all tasks can migrate according migration policy of vanilla kernel.

\newpage
\begin{figure}[htbp]
\centering
\includegraphics[width=\widefigure]{images/cpf_time/time_cpf_var_Xeon_i7.eps}
\caption{\figurecaption{variance of execution time of a sample on Xeon and i7: all task pinned compared vanilla}}
\label{fig:time_cpf_var_Xeon_i7}
\end{figure}

It is clear from the graph that the impact of cache miss on application's predictability is very significant on Intel i7
Performance degradation on Intel Xeon is not only due to migration pattern. The architectures of Xeon and i7 are very different, in particular for two 
apsects:

\begin{description}

\item[Inter-chip communication:] Intel i7 use the new Quick-path Interconnect that is a point-to-point processor interconnect developed by Intel to compete 
with HyperTransport. This first implementation of this bus achieve 25.6 GB/s, which provides exactly double the theoretical bandwidth of Intel's 1600 MHz 
FSB, that is the best performance obtainable with FSB. From datasheet, Intel Xeon E5440 use a FSB at 1333 MHz, therefore communication between chips are 
more faster on i7.

\item[Cache architectures:] Intel i7 uses three cache level and the LLC is inclusive and shared among all processors, while Intel Xeon use two separated 
cache blocks and each block is shared among only two cpus, that is cpus on the same dies. Thanks to inclusive LLC cache and MESIF protocol, snoop traffic
is greatly reduced.
\end{description}

According these facts, it is clear that communication between cores that belong to different dies is very expensive on Intel Xeon. On Intel Xeon, mixer0 
or mixer1 could find one buffer in L1 cache and other buffer in a bank of L2 that is shared by the cpu that have execute it. Mixer2 could find one buffer 
on L1 one cache, while other buffer is \textit{always} placed on bank of L2 that is not shared by the cpu that has executed Mixer2, therefore read 
latencies are very high, because data are placed on different dies. On i7, instead, mixer2 could find one buffer in L1 cache and other buffer in L3 cache 
that is shared among all cores and it is inclusive, therefore to read data is less expensive on i7.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Task-affinity improvements}

Before to explain how the patch works, it is necessary to remember the concept of task-affinity. We say that two tasks have a task-affinity relationship if 
they share data and their execution depends upon reading or writing these data \cite{lcs}. In a producer-consumer application, the producer is the one that 
writes to the shared buffer, while the consumer is the one that reads it. The consumer depends on data generated by producer since it needs them in order 
to be able to run, therefore we say that the consumer has a task-affinity relationship toward the producer.

\begin{figure}[htbp]
\centering
\includegraphics[width=\widefigure]{images/taskaff-rel.eps}
\caption{\figurecaption{Task-affinity relationship between producer and consumer}}
\label{fig:taskaff-rel}
\end{figure}

Each task is provided with a linked list called \textit{taskaffinity\_list} that contains all tasks to which it has task-affinity relationship, in briefly 
all its producers. To insert and delete a task in a \textit{taskaffinity list} two new system calls are provided:

\begin{description}

\item[sched\_add\_taskaffinity:] This system call adds a dependency to the current task, i.e. the task that issued the call. It receives, as parameter, the 
pid of the task the current one will be dependent upon.

\item[sched\_del\_taskaffinity:] When a task does not want anymore to use the task-affinity mechanism, it is possible to remove it through this call.
As in the case of inclusion of dependency, it suffices to pass as parameter the pid of the task one wants not to follow anymore.

\end{description}

Thanks to these system calls, the scheduler "knows" which tasks have task-affinity and, in this way, it can schedule consumers after producers in order to 
enforce reuse of cache memory.

The task-affinity logic influences wake up and migration of a task. As we have previously seen, in \textit{try\_to\_wake\_up} the choice of cpu where to 
put the to be waken task is made by \texttt{select\_task\_rq\_rt} TODO ref snip. This function is modified in this way: given the input task $p$, the 
function doesn't call \texttt{find\_lowest\_rq} but it loops for all element present in the $p$'s \textit{taskaffinity list} and build a mask, called 
\textit{affinity\_mask}, with cpus that have executed a task present in $p$'s \textit{taskaffinity list}. Finished the loop, the function returns the cpu 
with the runqueue that have the lowest number of Real-Time tasks. Only if the mask is empty, the function calls \texttt{find\_lowest\_rq} to choose a cpu
in the standard way. 

In the current version of task-affinity, a task that "respect" task-affinity, that is a task that was enqueued according to its task-affinity 
relationships, isn't able to migrate. In plain words, \texttt{push\_rt\_task} and \texttt{pull\_rt\_task} can't move tasks that "respect" task-affinity.

The aim of this policy is clear: when a task wakes up, the policy tries to select the best cpu for that task and, if it finds it, it blocks the task on the 
best runqueue until the task's execution. For this reason the key point of task-affinity logic is the \texttt{select\_task\_rq\_rt}. In the optimal case, 
producers and consumers will be executed subsequenlty always on the same cpu.

Nevertheless, in practice, the choosen cpu for $p$ is next to never the optimal cpu. The reason is very simple. The choice of the best 
cpu, and the enqueuing of task are performed in different moments. When \texttt{select\_task\_rq\_rt} is called, it doesn't held any lock. During the loop, 
the function has to read what is the content of different runqueues present in the system, these reads are not synchronized. When cpu is selected, 
\texttt{select\_task\_rq\_rt} returns, \texttt{try\_to\_wake\_up} \textbf{takes a lock} on choosen runqueue, in order to call \texttt{activate\_task} 
to perform the enqueuing of task. From when \texttt{select\_task\_rq\_rt} selects cpu to when lock is taken, a task with equal or higher priority than 
$p$'s priority can be inserted in the selected runqueue, in this way, the next task that will be executed won't be $p$. 

The current version of task-affinity ensures a weak concept of temporal locality because it doesn't ensure, when it is possible, that the next task executed
after a producer is a consumer. Another problem of the current version of task-affinity is the migration policy. It is not quite flexible. Pull and push 
functions mantain the system balanced, and guarantee that every cpu executes always the higher priority Real-time task present in its runqueue.
Therefore, the deny to pull and push can improve predictability of the application and can degrade singnificantly the throughput of the application.
Predictability is an important aspect for Real-time systems, but if we have a very bad throughput we don't exploit the potentiality of multicore platforms.

The aim of the patch developed it to improve the concept of temporal locality in order to execute a consumer immediately afteward a producer, when it is 
possible and to improve the migration policy in order to use also the functions involved in the migration mechanism to exploit the concept of task-affinity.
Furthermore, the patch make task-affinity more robust synchronizing access at data structures used. With this patch we try to improve throughput and 
predictability of the application. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Patch structure}

Proposed patch is divided in two parts. The first part improves the temporal locality, while the second part introduces mechanisms to synchronize access to 
task-affinity data structures.

%-----------------------------------------------------------------------------
\subsection{Temporal locality}

To ensure that a consumer will be the next executed task after a producer, it is necessary to change what \texttt{select\_task\_rq\_rt} "see". As I 
previously said, during its loop, \texttt{select\_task\_rq\_rt} check for cpu that \textbf{have executed} a task in $p$'s \textit{taskaffinity list}. 
It means that, in that moment, those cpus could be executing a task that it is not a producer and then, L1 cache could be already dirty. For this reason, 
at each runqueue was added a field named \texttt{last\_tsk} that contains the last task executed in a runqueue. This field is updated at each context switch 
if the next task to be executed is different from idle. In this way, if current task on runqueue is not idle, this field represents, the task in execution. 

With this additional field, \texttt{select\_task\_rq\_rt} "knows" which is the task currently executed on each runqueue. In this way, cpus that during 
\texttt{select\_task\_rq\_rt} are executing a task that is not in $p$'s \textit{taskaffinity list} are not inserted in \texttt{affinity\_mask}.

This change is not enough. Consider this situation: two different cpus that we call CPU\_A and CPU\_B are executing two different istances of 
\texttt{try\_to\_wake\_up}. Respectively, they are called for task $p$ and task $q$: the former has task-affinity relationship, the latter is a generic 
Real-time task, both tasks have the same priority. Suppose that the current task on CPU\_A is a task in $p$'s \textit{taskaffinity list} and then 
\texttt{select\_task\_rq\_rt} choose CPU\_A for $p$. Suppose that \texttt{try\_to\_wake\_up} that wakes up $q$ chooses CPU\_A and enqueue task $q$ on 
runqueue of CPU\_A. Task $p$ is not still enqueued, therefore when it will be enqueued, it will be preceeded by $q$ and then the next task that will
be executed on CPU\_A is $q$.

\begin{figure}[htbp]
\centering
\includegraphics[width=\widefigure]{images/enq_head.eps}
\caption{\figurecaption{Enqeue on head}}
\label{fig:enq_head}
\end{figure}

To resolve this problem, I have modified enqueuing of task in this manner: a task that "respects" task-affinity is enqueued on the top of a runqueue and not 
on tail. 

In this way, if two Real-time tasks are on the same runqueue and have the same priority, but one of them "respects" task-affinity, the next task that 
will be executed is the task that "respects" task-affinity. 

Until now we have modified the logic present in \texttt{try\_to\_wake\_up}. A carefully reader, will have noted that with this strategy a task with 
task-affinity can move up a task without task-affinity only if the latter is enqeued \textit{before} the former.

To resolve this problem, migration mechanism is used. When the \texttt{try\_to\_wake\_up} has enqueued task $p$, it calls \texttt{task\_woken} in order to
checks if $p$ can be executed on the selected runqueue or not. If on runqueue there is a task with priority equal or higher than the $p$'s priority and 
this task precede $p$, \texttt{push\_rt\_task} is called and $p$ can be pushed on another cpu. To select the cpu where to push $p$, the same mechanism used 
in \texttt{select\_task\_rq\_rt} is adopted. Therefore, $p$ will be pushed where it is in execution a task in $p$'s \textit{taskaffinity list}, if it is 
impossible standard push criteria are adopted and $p$ will be pushed on a cpu that executing a task with lower priority than $p$. Since now also 
\texttt{push\_rt\_task} is involved in task-affinity logic, and it use the same mechanism used in \texttt{select\_task\_rq\_rt},  the mechanism used to 
select taskaff TODO mettere il nome della funzione usata nella patch della find\_lowest. Obviously, in order to push a task that "respect" task-affinity, 
it is necessary insert it in a \textit{pushable list}. 

%-----------------------------------------------------------------------------
\subsection{Synchronization}

In the current version of task-affinity, accesses to data structures used to manage task-affinity are made by user or by kernel. The resource that must be
synchronized is \texttt{taskaffinity\_list} TODO del followme non sono sicuro

\begin{description}

\item[Access from user space:] User can access to task-affinity data structures using syscalls \texttt{sched\_add\_taskaffinity}, and 
\texttt{sched\_del\_taskaffinity}. These functions access to pid of the task received in input in synchronized way, because they using the read-write lock
tasklist\_lock that protects the kernel internal task list. For this reason, at every moment, only one instance of these syscalls can modify task-affinity 
data structure of a task.

\item[Access from kernel space:] Here the situation is more complex. There are two functions that can access to task-affinity data structures, they are:
\texttt{task\_affinity\_notify\_exit} and \texttt{select\_task\_rq\_rt}. The former function frees task-affinity data structures and it is called when a 
task is exiting. During this phase, all resources used by a task, pid included, are deallocated therefore, when \texttt{task\_affinity\_notify\_exit} is 
called, \textit{tasklist\_lock} is acquired. This is not enough because in that moment \texttt{select\_task\_rq\_rt} can access to task-affinity data 
structure of the exiting task, for this reason another layer of synchronization is needed. To resolve this problem, each task has its own read-write lock 
named \textit{taskaff\_lock} to protect its task-affinity data structures. 

\end{description}

%In figure are represented the two step of sycnhronization. In step (a) syscalls try to acquire \textit{tasklist\_lock} in order to avoid to access to a 
%\textit{taskaffinity list} of an exiting task. In step (b) all functions that want to access to task-affinity data structure of a certain task, must 
%acquire the \textit{taskaff\_lock} that protect those structures. A read-write lock was choosen because reads on task-affinity data structures are 
%most frequently than writes. Reads are performed by \texttt{select\_task\_rq\_rt} while writes are executed only by the syscalls and 
%\texttt{task\_affinity\_notify\_exit}.

