In the quest for the highest CPU performances, hardware developers have to deal with a difficult dilemma. On one hand, Moore's Law does not apply to 
computational power any more, that is, computational power is no longer doubling every 18 months as in the past. On the other hand, power consumption 
continues to increase more than linearly with the number of transistors included in a chip, and Moore's Law still holds for the number of transistors 
in a chip.
Several solutions have been adopted to solve this dilemma. Some of them try to reduce the power consumption by sacrificing computational power,
usually by means of frequency scaling, voltage throttling, or both. Other solutions try to increase the Instruction Level Parallelism (ILP) inside a 
processor, in order to get more computational power from the CPU without increasing power consumption. But nowadays the penalty of a cache miss 
(which may stall the pipeline) or of a miss-predicted branch (which may invalidate the pipeline) has become way too expensive.
Already in the early 2000's, it was clear that the most effective way to increase computational power and reduce power consumption was to parallelize
task execution. For this reason Simultaneous MultiThreading (SMT) was introduced. It is a technology that allowed to execute 
concurrently two threads on the same CPU. Nowadays it was introduced multicore
technolgy that consist of N superscalar processors put in the same chip. This tecnology allows a great parallelization of task execution.
With reference to memory organisation, multiprocessor systems are classified into two groups:

\begin{itemize}
	\item Centralised Shared Memory Architectures: in this architecture, there are multiple cores connected to a single shared memory. If all cores are
	      equal this architecture is called simmetric multiprocessor (SMP)
	\item Distributed Memory Architecture: in this architecture each processor has its own memory module and memory access time depends on the memory 
	      location relative to a processor. Non-uniform memory access (NUMA) processor are included in this category of architecture
\end{itemize}

Multicore architectures have been adopted by most chip manufacturers. Dual-core chips are commonplace, and numerous four and eight core options exist. 
In the coming years, per-chip core counts will continue to increase: Intel has claimed that it will release 80-core chips as early as 2013. 
The shift to multicore technologies is a watershed event, as it fundamentally changes the "standard" computing platform in many settings to be a 
multiprocessor.

Even many embedded systems are starting to adopt multicore architectures, because theese processors can provide a large increment of computational power,
with small increase in power consumption, that is a very important aspect for this type of systems.
But there is an obstacle to the use of these architectures in this sector and in particular in Real-Time systems.
In most multicore platforms, different cores share onchip caches. Imagine this situation: there are three Real-time tasks: A, B and C. 
A uses 512KB of memory, B uses 768KB and C uses 256KB. Our platform has a dual-core chip. Shared onchip cache is of 1MB.
There are two possible case of scheduling. In first case A and C (or B and C) are scheduled. There is enough space in cache to alloc their resources, 
and it is good. In the second case A is scheduled with B: cache trashing occurs. Performance of two tasks could get worse respect the previous case, because 
there isn't any guarantee that A or B may find data in shared cache and, furthermore, it is impossible to predict A or B duration, because if A is 
scheduled with B, it will have a certain duration. Instead, if A is scheduled with C, it will have another duration.
In other words: a task's duration depends from which other task is scheduled with it and, for this reason, using common Real-Time scheduling algorithms in 
multicore systems, well developed techinques for timing analysis of embedded software used in single processor systems are no loger useful, therefore 
new techinques are needed to estimate the worst-case execution time (WCET) of Real-Time tasks for this kind of platforms. 
It is clear that the scheduler plays an important role to improve performance and predictability of the applications. Nowadays, it is 
important to develop scheduling algorithms "cache-aware", that is a scheduler
that, to choose on which cpu to put a task, it consider  about how scheduled 
tasks use cache memory, in order to avoid cache trashing.
This thesis is the prosecution of the work carried out by Lucas De Marchi. He has tried to make the Linux Real-Time scheduler cache-aware
introducing the concept of taskaffinity. In this work, I have tried to improve the concept of taskaffinity and I 
have studied the behaviour of this mechanism on different multicore architectures.

\section{State of the art}
\label{sec:StateOfTheArt}

Although the problem to design "cache aware" scheduling algorithms is an old and well-known problem for over 20 years and multicore processors are 
largely used, nowadays operating systems don't implement this kind of algorithms and in literature there are only a few works that study this problem. 
The most of recent research works related to this argument consist of profiling activities with the aim to demonstrate and build a model that show how
an unfair cache sharing between concurrent threads may slow down them and cause sub-optimal throughput, cache trashing and, in some cases, thread 
starvation for threads that fail to occupy sufficient cache space to make good progress.
The first well-documented work related to this kind of scheduling algorithms, was developed at the Stanford University.
At the end of 80's, the Computer Systems Laboratory at Stanford University designed a prototype of a shared-memory multiprocessor called DASH. 
Its architecture was very similar to that used in modern SMP processors. DASH was able to incorporate up to 64 high-performance RISC microprocessors. 
In order to exploit full potentiality of this machine, they developed a suitable
runtime system to use with DASH, and they designed the COOL language. 
It was an extension of C++, that introduced some statements to facilitate expression of medium to large grain parallelism and to define which was the 
\textit{data reference patterns} of the program.
The COOL compiler was able to automatically extract fine-grain parallelism for architectures that, like DASH, supported such a level of concurrency,
and extract information about use of cache made by applications. Using these informations, the runtime system could ensure parallelism desired by 
programmer and try to reduce cache miss rate of each task, because this system "knew", for each task, which were the objects referenced by it, so 
it distributed tasks and objects in order to make them close.
In plain words, using additional informations provided by the programmer and exploiting the principle of data locality, the runtime system decided where to 
allocate objects and it assigned a task to a CPU that contained objects referenced by it in its cache. The COOL project shows how the smart use of cache 
is a problem that involves all aspect of software engineering, from the compiler to scheduler and memory management system.

Interesting research activities made in recent years exploit another strategy. They don't introduce new programming language or sophisticated runtime 
environment, but they implement a raw profiler that, at runtime, it infers how much cache space a task requires, in order to infer which 
tasks could cause cache trashing if they would be executed concurrently.
To make this job, the profiler executes a periodical tuning phase in which it analyzes miss rate of each task, in this way it is possible understand the 
amount of space used by a task. According to these informations, two or more
task are scheduled on different CPUs only if 
they don't cause cache trashing. These works are not effective as COOL project, but they present good results with SPEC2000 and 
LITMUS \footnote{It is a Linux-based testbed developed by them that supports multiprocessor real-time scheduling policies within Linux} benchmarks, 
furthermore this kind of works was experimented with good outcome also in embedded systems.


\section{Objectives of this thesis work}
\label{sec:ObjectiveOfThesis}

The first goal of this thesis is to improve the weaknesses of current version of task-affinity. As described in \cite{lcs}, functions used to perform 
migration of tasks are not involved in task-affinity logic. Furthermore, 

migration policy adopted in 
task-affinity is not particular effective, 


The first goal of this thesis is to investigate which is the behaviour of task-affinity on different SMP architectures.

TODO imbastire discorso migration


%The version of task-affinity developed in \cite{lcs} try to improve determinism, reducing L1 miss rates of the application. Cache misses are only one 
%possible factor, there are many more hardware factors that influences predictability of applications, such as: inter-chip communications, cache 
%architectures, hardware prefetching etc. We want to test task-affinity on different architectures, in order to analyze how task-affinity manages these 
%factors and how much performance are improved.

%We strongly believe that being able to re-use code that was already extensively tested is a goal to pursue. Thus, we must try to give solutions that are 
%ompatible with the current scheduler implementation %and overall behavior. Moreover, the Linux development model %imposes that code entering the kernel 
%(i) does what it has to do with little %or no side-effects, being unperceivable for scenarios it does not cover; %(ii) be made in a gradual pace, without 
%throwing code away, unless with %big and proved reasons; (iii) be efficiently implemented; and (iv) be reviewed %and accepted by the community.
%Linux and other Unix-like Operating Systems already provide a concept %of processor affinity with which is possible to bound a task to a %certain CPU. 
%Linux provides even a mechanism to isolate a CPU so just %a limited number of tasks could run on it. Even though this approach %solves some problems of 
%real-time tasks it is not a desirable solution %because computation power will be underutilized. As said in Sec. 1.1, %pinning real-time tasks to specific 
%CPUs is an improvement only if there %are just a few of them and plenty of normal tasks that could run on other %processors.

%With the analytical results and the embedded world in mind, besides %little solutions presented on the analysis step, we propose a bigger change
%to the Linux scheduler, though not throwing away present code. This %change is based on results got from a benchmark, built with this work,
%which simulates a possible substitution of hardware blocks by real-time %tasks in software. This application uses cooperative threads to produce
%the final results, which is shown to be not optimally dealt with in current %Linux scheduler. Then it is created in Linux the concept of affinity among
%threads. It turned out that the current state of real-time scheduling %in Linux is optimized much more for competitive threads rather than
%cooperative. %Therefore, the goals of this thesis can be summarized as:


%TODO negli obbiettivi:
%
%the first goal of this thesis is to investigate how task-affinity behaves on different architectures. questo \'e stato fatto perch\'e as showed in molka
%le cache scassano il cazzo sono complicate devo dire che la task-aff \'e troppo semplicistica ecc.. ... riempi.. for this reasons it is necessary how taskaffinity 
%

\section{Organization of the thesis}
\label{sec:OrganizationThesis}

